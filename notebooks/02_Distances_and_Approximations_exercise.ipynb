{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d685c48e8dd2865",
   "metadata": {},
   "source": [
    "# Distance and Approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b6fc9bc2e0e32c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:11:34.280144Z",
     "start_time": "2024-05-22T14:11:33.825858Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sktime.datasets import load_UCR_UEA_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f4396c154c611",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99b9cd0078e3c1",
   "metadata": {},
   "source": [
    "---\n",
    "Load the Cylinder-Bell-Funnel (CBF) dataset either using the `load_UCR_UEA_dataset` function or by loading the data from the `data` folder. \n",
    "\n",
    "If you use the `load_UCR_UEA_dataset` function, be sure to set the `return_type` parameter to `numpy3D`. If you load the data locally, you can use the `np.load` function.\n",
    "\n",
    "Be sure to load both the data and the classes. Store the data in a variable `X` and the classes in a variable `classes`. X should contain 930 univariate time series of length 128. The classes should contain the class labels for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0d8e60cd7d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f17e889176ec697",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Store the first two time series of the dataset in the variables `ts1` and `ts2`. Plot the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1cfdf4d969df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8942878349140d",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31d64a83b3ffd34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:11:35.930473Z",
     "start_time": "2024-05-22T14:11:35.921214Z"
    }
   },
   "outputs": [],
   "source": [
    "from sktime.distances import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6adc9a617fb46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Calculate all the distances you saw in the lecture between `ts1` and `ts2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2639040e03edce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffb353207fbae5e",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76bbb3fd5f4269b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:11:37.270686Z",
     "start_time": "2024-05-22T14:11:37.263948Z"
    }
   },
   "outputs": [],
   "source": [
    "from sktime.alignment.dtw_numba import AlignerDtwNumba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753703bb5c34ffc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Visualize the alignments for the DTW distance, using a sakoe-chiba window of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27dc3e5bbd8bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3be04ccb5ddbfb32",
   "metadata": {},
   "source": [
    "## Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3effaf8e6286145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:11:38.017999Z",
     "start_time": "2024-05-22T14:11:37.993217Z"
    }
   },
   "outputs": [],
   "source": [
    "from sktime.transformations.series.paa import PAA\n",
    "from sktime.transformations.series.sax import SAX\n",
    "from pyts.approximation import DiscreteFourierTransform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "def dft_inverse_trasform(X_dft, n_coefs, n_timestamps):\n",
    "    # https://pyts.readthedocs.io/en/latest/auto_examples/approximation/plot_dft.html\n",
    "    n_samples = X_dft.shape[0]\n",
    "    if n_coefs % 2 == 0:\n",
    "        real_idx = np.arange(1, n_coefs, 2)\n",
    "        imag_idx = np.arange(2, n_coefs, 2)\n",
    "        X_dft_new = np.c_[\n",
    "            X_dft[:, :1],\n",
    "            X_dft[:, real_idx] + 1j * np.c_[X_dft[:, imag_idx],\n",
    "                                            np.zeros((n_samples, ))]\n",
    "        ]\n",
    "    else:\n",
    "        real_idx = np.arange(1, n_coefs, 2)\n",
    "        imag_idx = np.arange(2, n_coefs + 1, 2)\n",
    "        X_dft_new = np.c_[\n",
    "            X_dft[:, :1],\n",
    "            X_dft[:, real_idx] + 1j * X_dft[:, imag_idx]\n",
    "        ]\n",
    "    X_irfft = np.fft.irfft(X_dft_new, n_timestamps)\n",
    "    return X_irfft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b30b9ac95b6bd",
   "metadata": {},
   "source": [
    "---\n",
    "As you probably saw, the time series are very noisy. Your goal here is to denoise them, without losing too much information. If you did things correctly, you should be able to see that the two time series you analized are quite similar. Now store the fourth time series (so `i=3`) in the variable `ts3` and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16e37222d8299d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677aa93b7c5e4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9022d85d8dc7fcab",
   "metadata": {},
   "source": [
    "---\n",
    "You should be able to see that ts1 and ts2 are quite similar, while ts3 is quite different.\n",
    "\n",
    "Your goal here is to approximate `ts1`, `ts2` and `ts3` using one method between PAA, SAX, DFT, such that you remove the noise but maintain the general shape of the time series. At the end `ts1_approx` and `ts2_approx` should still be similar, while `ts3_approx` should be quite different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82dc7e52d64c45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Approximate `ts1`, `ts2` and `ts3` using one method between PAA, SAX, DFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97640bc6412d9327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5bc27f2b845e301",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2. Plot the original time series and the approximated time series (`ts1` with `ts1_approx`, `ts2` with `ts2_approx`, `ts3` with `ts3_approx`). Then plot all the approximations in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fcaec04abeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17ae386fde7bd87a",
   "metadata": {},
   "source": [
    "---\n",
    "3. Evaluate the results qualititatively by looking at the plots, and by computing the dtw distance between `ts1_approx` and `ts2_approx`, and between `ts1_approx` and `ts3_approx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4912243102182b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc26a366305ff59",
   "metadata": {},
   "source": [
    "---\n",
    "Now approximate the entire dataset (`X`) using PCA. You can use the `PCATransformer` from sktime. Find the number of components that explain at least 70% of the variance. Then, approximate the dataset using that number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb78e6c7de46cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "225380cb69f8be8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot `ts1` and its approximation using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c9d6a8a0a663a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fcf06c60f66d07c",
   "metadata": {},
   "source": [
    "---\n",
    "Now use `Tabularizer`and `PCA` with 2 components to compress the dataset. Then, plot the compressed dataset. Use the class labels to color the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c995b835e03969",
   "metadata": {},
   "source": [
    "## Distance-based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21de1eb978567377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:28:16.640641Z",
     "start_time": "2024-05-22T14:28:16.631478Z"
    }
   },
   "outputs": [],
   "source": [
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0b5bb6a262eee",
   "metadata": {},
   "source": [
    "---\n",
    "In the following, we used a very simple SummaryTransformer to convert the time series into a tabular form and used a KNN classifier to classify the time series.\n",
    "\n",
    "For this exercise you want to maximize accuracy and also runtime. Your goal is to beat our SummaryTransformer + KNN classifier. You can use any normalization, approximation and feature extraction method you want. The only constraint is that you should use the same classifier. You can use KNN directly on the time series (KNeighborsTimeSeriesClassifier), or perform a tabular conversion in some way (as we did for the SummaryTransformer) and then use a standard KNN from sklearn (KNeighborsClassifier).\n",
    "\n",
    "- Can you find a model that is more accurate?\n",
    "- Can you find a model that is faster and more accurate?\n",
    "- Can you find a model that is faster, uses less features and is more accurate?\n",
    "\n",
    "**Note 1:** You can use the `%%time` magic command to measure the time it takes to run a cell. You have to count all the transformations and the training/test time.\n",
    "\n",
    "**Note 2:** Training has to be done only on the training set, and the test set should be used only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f53d90df08d16462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:30:42.253256Z",
     "start_time": "2024-05-22T14:30:42.140586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1, 128), (30,), (900, 1, 128), (900,))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = load_UCR_UEA_dataset(\"CBF\", split=\"TRAIN\", return_type=\"numpy3D\")\n",
    "X_test, y_test = load_UCR_UEA_dataset(\"CBF\", split=\"TEST\", return_type=\"numpy3D\")\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9ce6cad2da2c2ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:30:43.162193Z",
     "start_time": "2024-05-22T14:30:43.159360Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = SummaryTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1df33ca6379a909b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:30:53.345983Z",
     "start_time": "2024-05-22T14:30:49.883224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.27 s, sys: 14.4 ms, total: 3.29 s\n",
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_features_train = summary.fit_transform(X_train)\n",
    "X_features_test = summary.transform(X_test)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_features_train, y_train)\n",
    "y_pred = knn.predict(X_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ab6026c75b0e4655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:30:53.354900Z",
     "start_time": "2024-05-22T14:30:53.347299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6633333333333333"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "279a6f44abb0db12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T14:33:36.619746Z",
     "start_time": "2024-05-22T14:33:36.616813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"n_features:\", X_features_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd410744878e49c3",
   "metadata": {},
   "source": [
    "---\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9d1e0872fe8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce44d50873ef646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4898dd0c8b9c462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2db84e6a6c3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caf9d4a47b231e78",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab610f62c349e71a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "What's the effect of dynamic time warping when comparing time series? Was it useful in this dataset? Why?\n",
    "\n",
    "Which approximation method did you use? Did it work well? Why?\n",
    "\n",
    "Did approximation make the time series more similar?\n",
    "\n",
    "What can you grasp from the 2d representation of the dataset using PCA?\n",
    "\n",
    "What was the best method you found for classification? Why do you think it worked better than the others?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
